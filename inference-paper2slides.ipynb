{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7166359,"sourceType":"datasetVersion","datasetId":4118869}],"dockerImageVersionId":30616,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q transformers bitsandbytes datasets\n!pip install evaluate rouge-score\n!huggingface-cli login","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading Model","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nimport torch\n# base_model = \"meta-llama/Llama-2-7b-chat-hf\"\nbase_model = \"abhi757/llama-2-7b-chat-paper-to-slides2\"\n# Reload tokenizer to save it\n\ncompute_dtype = getattr(torch, \"float16\")\n\nquant_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=compute_dtype,\n    bnb_4bit_use_double_quant=False,\n)\n\n# Loading Llama 2 model\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model,\n    quantization_config=quant_config,\n    device_map={\"\": 0}\n)\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\ntokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"","metadata":{"execution":{"iopub.status.busy":"2023-12-12T11:17:55.052301Z","iopub.execute_input":"2023-12-12T11:17:55.053385Z","iopub.status.idle":"2023-12-12T11:19:41.730142Z","shell.execute_reply.started":"2023-12-12T11:17:55.053341Z","shell.execute_reply":"2023-12-12T11:19:41.729068Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/661 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d6ad6b803244a3fb589516721e3a159"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b6cb9a512b046b2a67b8174d2debd06"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0fb3206e2294e6a95eaaebcba695b8e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4dcc95a446de40d7a4a8655a9f6c07da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5188a148163b4d5cac01c0f9565b8309"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/3.59G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b964fde1ac14196a97c04a59e51f157"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3506198736904071bee74d33844383e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/183 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e12cf2de33e47bfbab541e63ff8ef97"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.71k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3812272d3a704b0fb6a89de343e4fd3a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36523167e93d40bbb43c489e6124f4da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/437 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e0906e5e2994a4a92d3e126c41d440b"}},"metadata":{}}]},{"cell_type":"markdown","source":"## Prompt for Topic and Bullet point generation","metadata":{}},{"cell_type":"code","source":"system_prompt = \"\"\"\n<s>[INST] <<SYS>>\nYou are a helpful, respectful and honest assistant for labeling topics and generating summaries.\n<</SYS>>\n\"\"\"\nmain_prompt = \"\"\"\n[INST]\nI have a topic that contains the following documents:\n{documents}\n\nBased on the information about the topic above, you have two tasks.\nTask-1: Please create a short label of this topic. Make sure you to only return the label and nothing more.\nTask-2: Please create a short summmary of this topic describing the steps in the documents. Make sure that the you do not report more than six sentences in the list. Make sure to report the summary in a list of sentences. Make sure that each sentence does not exceed 10 words. Make sure to only return the list of sentences and nothing more.\n\nPut this data into a JSON list with keys \"label\" and \"summary\".\n[/INST] \n\"\"\"\nprompt = system_prompt+main_prompt\n\n# def add_prefix(example):\n#     example[\"text\"] = prompt.format(documents=example['documents'])\n#     return example\n# dataset = dataset.map(add_prefix)","metadata":{"execution":{"iopub.status.busy":"2023-12-12T11:47:07.071607Z","iopub.execute_input":"2023-12-12T11:47:07.072274Z","iopub.status.idle":"2023-12-12T11:47:07.077859Z","shell.execute_reply.started":"2023-12-12T11:47:07.072238Z","shell.execute_reply":"2023-12-12T11:47:07.076646Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"from transformers import pipeline,logging\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer,max_length=2048,temperature = 0.1)","metadata":{"execution":{"iopub.status.busy":"2023-12-12T11:20:19.710481Z","iopub.execute_input":"2023-12-12T11:20:19.711437Z","iopub.status.idle":"2023-12-12T11:20:38.051085Z","shell.execute_reply.started":"2023-12-12T11:20:19.711398Z","shell.execute_reply":"2023-12-12T11:20:38.050026Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Loading Dataset and reformat","metadata":{}},{"cell_type":"code","source":"import xml.etree.ElementTree as ET\nimport glob, os\nimport json\n\ndef get_section_details(file):\n  tree = ET.parse(file[0])\n  root = tree.getroot()\n\n  sections = []\n\n  for neighbor in root.findall('.//{http://www.tei-c.org/ns/1.0}div'):\n    i=0\n    section_temp = {}\n    content=[]\n    for head in neighbor:\n      if (i==0):\n        # print('Section Title: ', head.text,\"\\n\")\n        section_temp['title'] = head.text\n        # print(\"i = \",i)\n      else:\n        content.append(head.text)\n      i+=1\n    section_temp['content'] = content\n    if content!=[] and section_temp['title']!=None:\n      sections.append(section_temp)\n  return sections\n\ndef get_slide_details(file):\n  tree = ET.parse(file[0])\n  root = tree.getroot()\n\n  slides = []\n  slide_count=0\n\n  for slide in root.findall('div'):\n    slide_count+=1\n    slide_temp = {}\n    slide_temp['id'] = slide_count\n\n    i=0\n    content=[]\n    for head in slide:\n      if (i==0):\n        # print('Section Title: ', head.text,\"\\n\")\n        slide_temp['title'] = head.text\n      else:\n        content.append(head.text)\n      i+=1\n    slide_temp['content'] = content\n    if(content):\n      slides.append(slide_temp)\n\n  return slides\n\nimport numpy as np\n\ndef reformat_sections_for_llm(sections):\n  para_format  = \"title: {title}, content: {content}\"\n  for section in sections:\n    for i in range(len(section['content'])):\n      section['content'][i]=para_format.format(title=section['title'],content=section['content'][i])\n  return sections\n\ndef get_section_partition_indices(token_lens,token_thresh=400):\n  token_lens = np.cumsum(token_lens)\n  partition_ids = []\n  for i in range(len(token_lens)-1):\n    if (abs(token_lens[i]-token_thresh) < abs(token_lens[i+1]-token_thresh)):\n      partition_ids.append(i) \n      token_lens-=token_lens[i]\n  partition_ids.append(len(token_lens)-1)\n  return partition_ids\n    \ndef partition_section(section_content,token_thresh = 400):\n  token_lens = [len(tokenizer.tokenize(para)) for para in section_content]\n  partition_ids = get_section_partition_indices(token_lens,token_thresh)\n  start_id = 0\n  documents = []\n  for id in partition_ids:\n    documents.append(section_content[start_id:id+1])\n    start_id = id+1\n  return documents\n\ndef gen_documents_for_slides(sections):\n  documents=[]\n  max_token_thresh = 500\n  token_min_thresh = 100\n  for section in sections:\n    num_tokens = len(tokenizer.tokenize(\"\".join(section['content'])))\n    if num_tokens<max_token_thresh and num_tokens > token_min_thresh:\n      documents.append(section['content'])\n      # Remember to append section title\n    elif num_tokens>max_token_thresh:\n      section_documents = partition_section(section['content'])\n      for docs in section_documents:\n        if len(tokenizer.tokenize(\"\".join(docs)))>token_min_thresh:\n          documents.append(docs)\n  return documents\n","metadata":{"execution":{"iopub.status.busy":"2023-12-12T11:20:38.053128Z","iopub.execute_input":"2023-12-12T11:20:38.054242Z","iopub.status.idle":"2023-12-12T11:20:38.078606Z","shell.execute_reply.started":"2023-12-12T11:20:38.054200Z","shell.execute_reply":"2023-12-12T11:20:38.077727Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def get_prediction(pipe,slide_base_documents,prompt):\n    prompt_modified = [prompt.format(documents=docs) for docs in slide_base_documents]\n    output = pipe(prompt_modified)    \n    result = [output[ind][0]['generated_text'].replace(prompt_modified[ind],\"\") for ind in range(len(output))]\n    return result\n\ndef convert_to_json(result):\n    result_json=[]\n    i=0\n    for res in result:\n        try:\n            result_json.append(json.loads(res.replace('\\n',\"\")))\n        except:\n            print(\"Slide No. \", i, 'not in correct format')\n        i+=1\n    return result_json\n\ndef evaluate_metric(prediction, target,metric):\n    \"\"\"\n    prediction : List of json objects with keys={'label','summary'}\n    target: List of dictionaries with keys = {'id','title','content'}\n    \"\"\"\n    # Concatenate all predicted slides into a single text\n    slide_all_predicted_list=[]\n    for res in prediction:\n        slide_all_predicted_list.append(res['label'])\n        slide_all_predicted_list.append(\" \".join(res['summary']))\n\n    slide_all_predicted = \" \".join(slide_all_predicted_list)\n    \n    # Concatenate all target slides into a single text\n    slide_all_target_list=[]\n    for i in range(1,len(target)):\n        slide_all_target_list.append(target[i]['title'])\n        slide_all_target_list.append(\" \".join(target[i]['content']))\n\n    slide_all_target = \" \".join(slide_all_target_list)\n    results = metric.compute(predictions=[slide_all_predicted], references=[slide_all_target])\n    \n    return results\n\ndef save_generated_slides(paper_index,predicted_slides):\n    save_path = '/kaggle/working/fine-tune-prediction/'+str(paper_index)+'.json'\n    with open(save_path, 'w') as file:\n        for sample in predicted_slides:\n          json.dump(sample, file)\n          file.write('\\n')","metadata":{"execution":{"iopub.status.busy":"2023-12-12T11:21:19.429260Z","iopub.execute_input":"2023-12-12T11:21:19.429674Z","iopub.status.idle":"2023-12-12T11:21:19.442190Z","shell.execute_reply.started":"2023-12-12T11:21:19.429639Z","shell.execute_reply":"2023-12-12T11:21:19.441127Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import evaluate\nrouge = evaluate.load('rouge')\nrouge_score_list = []\nprediction_list=[]\nfor paper_index in range(80,100):\n    # paper_index = 82\n    dir_name = '/kaggle/input/pdf2slides-dataset-100/dataset_0_99-20231206T111845Z-001/dataset_0_99/'+str(paper_index)+'/'\n    print(dir_name)\n    prefix = 'Paper'\n    file_paper = glob.glob(os.path.join(dir_name, prefix + '*.xml'))\n\n    prefix = 'slide'\n    file_slide = glob.glob(os.path.join(dir_name, prefix + '*.xml'))\n\n    sections = get_section_details(file_paper)\n    slides = get_slide_details(file_slide)\n\n    sections = reformat_sections_for_llm(sections)\n    slide_base_documents = gen_documents_for_slides(sections)\n\n    predicted_slides = get_prediction(pipe,slide_base_documents,prompt)\n    prediction_list.append(predicted_slides)\n    predicted_slides_json = convert_to_json(predicted_slides)\n    save_generated_slides(paper_index,predicted_slides_json)\n\n    rouge_score = evaluate_metric(predicted_slides_json, slides,rouge)\n    rouge_score_list.append(rouge_score)\n    print(rouge_score)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}